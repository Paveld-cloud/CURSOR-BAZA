# ============================================================
#   data.py ‚Äî –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω-–≤–µ—Ä—Å–∏—è (–ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º–∞)
#   –ü–æ–∏—Å–∫: strict code ‚Üí part ‚Üí OEM ‚Üí SEARCH_COLUMNS ‚Üí fallback
#   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º—É —Å—Ç–æ–ª–±—Ü—É image
#   Telegram Bot + Mini App —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
# ============================================================

import os
import re
import io
import json
import time
import asyncio
import logging
from typing import Dict, Set, List, Optional, Tuple

import pandas as pd
import aiohttp
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from zoneinfo import ZoneInfo

logger = logging.getLogger("bot.data")

# ---------------- CONFIG ----------------
try:
    from app.config import (
        SPREADSHEET_URL,
        SAP_SHEET_NAME,
        USERS_SHEET_NAME,
        DATA_TTL,
        SEARCH_COLUMNS,
    )
except Exception:
    SPREADSHEET_URL = os.getenv("SPREADSHEET_URL", "")
    SAP_SHEET_NAME = os.getenv("SAP_SHEET_NAME", "SAP")
    USERS_SHEET_NAME = os.getenv("USERS_SHEET_NAME", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    DATA_TTL = int(os.getenv("DATA_TTL", "600"))

    SEARCH_COLUMNS = [
        "—Ç–∏–ø",
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
        "–∫–æ–¥",
        "oem",
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å",
        "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
        "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
    ]

GOOGLE_APPLICATION_CREDENTIALS_JSON = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON")
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ---------------- GLOBAL ----------------
df: Optional[pd.DataFrame] = None
_last_load_ts: float = 0

_search_index: Dict[str, Set[int]] = {}
_image_index: Dict[str, List[str]] = {}

SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()

user_state: Dict[int, dict] = {}
issue_state: Dict[int, dict] = {}

ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# ============================================================
#                    NORMALIZATION
# ============================================================

def norm_code(val: str) -> str:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç –ª—é–±–æ–π –∫–æ–¥ –∫ —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –≤–∏–¥—É:
    - –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
    - O ‚Üí 0
    - —É–±–∏—Ä–∞–µ–º –≤—Å—ë –∫—Ä–æ–º–µ a-z0-9
    """
    s = str(val or "").strip().lower()
    s = s.replace("o", "0")
    return re.sub(r"[^a-z0-9]", "", s)


def norm_text(val: str) -> str:
    """–£–ø—Ä–æ—â—ë–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è."""
    return str(val or "").strip().lower()


def squash(val: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –ø–æ–ª–Ω–æ—Å—Ç—å—é."""
    return re.sub(r"[\W_]+", "", str(val or "").lower())


def tokenize(val: str) -> List[str]:
    return re.findall(r"[a-z0-9]+", str(val or "").lower())


# --- –í–ê–ñ–ù–û: –¥–ª—è Mini App (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å webapp.py) ---
def normalize(text: str) -> str:
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Mini-App.
    –£–±–∏—Ä–∞–µ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –æ—Å—Ç–∞–≤–ª—è–µ—Ç –±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø—Ä–æ–±–µ–ª—ã.
    """
    text = str(text or "").lower()
    text = re.sub(r"[^\w\s]", "", text)
    return text.strip()


# ============================================================
#                 GOOGLE SHEETS LOADER
# ============================================================

def gs_client():
    info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
    creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    return gspread.authorize(creds)


def load_sap_df() -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º SAP-–ª–∏—Å—Ç –¢–ê–ö, –∫–∞–∫ –æ–Ω –æ—Ç–æ–±—Ä–∞–∂—ë–Ω –≤ Google Sheets.
    –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ü–µ–Ω–∞–º–∏ –∏ —Ç–µ–∫—Å—Ç–∞–º–∏.
    """
    sh = gs_client().open_by_url(SPREADSHEET_URL)
    ws = sh.worksheet(SAP_SHEET_NAME)

    values = ws.get_all_values()
    if not values:
        return pd.DataFrame()

    headers = [h.strip().lower() for h in values[0]]
    df_new = pd.DataFrame(values[1:], columns=headers)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
    for col in ("–∫–æ–¥", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem"):
        if col in df_new.columns:
            df_new[col] = df_new[col].astype(str).str.strip().str.lower()

    if "image" in df_new.columns:
        df_new["image"] = df_new["image"].astype(str).str.strip()

    return df_new
# ============================================================
#                 INDEX BUILDERS (SEARCH + IMAGE)
# ============================================================

def build_search_index(df_: pd.DataFrame) -> Dict[str, Set[int]]:
    """
    –°—Ç—Ä–æ–∏—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - –∫–æ–¥
    - –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä
    - OEM –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä
    - —Ç–∏–ø, –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ, oem, –∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å
    –ü–æ–ª—è –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è —á–µ—Ä–µ–∑ SEARCH_COLUMNS.
    """
    index: Dict[str, Set[int]] = {}
    cols = [c for c in SEARCH_COLUMNS if c in df_.columns]

    for i, row in df_.iterrows():
        for col in cols:
            raw = str(row.get(col, "")).lower()

            # –î–ª—è –∫–æ–¥–æ–≤ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            if col in ("–∫–æ–¥", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"):
                key = norm_code(raw)
                if key:
                    index.setdefault(key, set()).add(i)

            # –¢–æ–∫–µ–Ω—ã –æ–±—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞
            for tok in tokenize(raw):
                index.setdefault(tok, set()).add(i)

    return index


# ------------------------------------------------------------
#                 IMAGE INDEX (strict match)
# ------------------------------------------------------------

def _image_tokens(url: str) -> List[str]:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    –ù–∞–ø—Ä–∏–º–µ—Ä:
    https://.../UZ000664.jpg ‚Üí ["uz000664"]
    """
    try:
        name = url.strip().lower().split("/")[-1]
        name = name.split("?")[0]
        name = name.split(".")[0]
        return tokenize(name)
    except Exception:
        return []


def build_image_index(df_: pd.DataFrame) -> Dict[str, List[str]]:
    """
    –ò–Ω–¥–µ–∫—Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:
    key = norm_code(—Ç–æ–∫–µ–Ω)
    value = —Å–ø–∏—Å–æ–∫ URL, –≥–¥–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ.
    """
    index: Dict[str, List[str]] = {}
    if "image" not in df_.columns:
        return index

    for _, row in df_.iterrows():
        url = str(row.get("image", "")).strip()
        if not url:
            continue

        tokens = _image_tokens(url)
        for tok in tokens:
            key = norm_code(tok)
            if len(key) >= 3:   # –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –º—É—Å–æ—Ä
                index.setdefault(key, []).append(url)

        # –¢–∞–∫–∂–µ –∫–ª—é—á = —Å–∫–ª–µ–µ–Ω–Ω–æ–µ –∏–º—è
        join_key = "".join(tokens)
        if len(join_key) >= 3:
            index.setdefault(join_key, []).append(url)

    return index


# ============================================================
#          LOAD + REFRESH (ensure_fresh_data)
# ============================================================

def ensure_fresh_data(force: bool = False):
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç DataFrame –∏ –∏–Ω–¥–µ–∫—Å—ã, –µ—Å–ª–∏ TTL –∏—Å—Ç—ë–∫.
    """
    global df, _search_index, _image_index, _last_load_ts

    need_reload = force or df is None or (time.time() - _last_load_ts > DATA_TTL)
    if not need_reload:
        return

    logger.info("üì• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ SAP-–¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Sheets...")
    df_new = load_sap_df()

    df = df_new
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)

    _last_load_ts = time.time()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫, –∏–Ω–¥–µ–∫—Å–æ–≤: search={len(_search_index)}, images={len(_image_index)}")
# ============================================================
#                       SEARCH ENGINE
# ============================================================

def search_exact(df_: pd.DataFrame, q: str) -> List[int]:
    """
    1) –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –ö–û–î–£ (norm_code)
    2) –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –ü–ê–†–¢ –ù–û–ú–ï–†–£
    3) –ï—Å–ª–∏ –Ω–µ—Ç ‚Äî —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ OEM –ü–ê–†–¢ –ù–û–ú–ï–†–£
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å—Ç—Ä–æ–∫.
    """
    key = norm_code(q)
    if not key:
        return []

    hits = []

    # --- 1) –∫–æ–¥ ---
    if "–∫–æ–¥" in df_.columns:
        for i, row in df_.iterrows():
            if norm_code(row.get("–∫–æ–¥", "")) == key:
                hits.append(i)

    if hits:
        return hits

    # --- 2) –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä ---
    if "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä" in df_.columns:
        for i, row in df_.iterrows():
            if norm_code(row.get("–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "")) == key:
                hits.append(i)

    if hits:
        return hits

    # --- 3) OEM –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä ---
    if "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä" in df_.columns:
        for i, row in df_.iterrows():
            if norm_code(row.get("oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "")) == key:
                hits.append(i)

    return hits


# ------------------------------------------------------------
#              INDEX MATCH (AND ‚Üí OR)
# ------------------------------------------------------------

def search_index(tokens: List[str]) -> Set[int]:
    """
    –ü–æ–∏—Å–∫ –ø–æ —Ç–æ–∫–µ–Ω–∞–º —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã–π search_index.
    –õ–æ–≥–∏–∫–∞:
    - –µ—Å–ª–∏ –≤—Å–µ —Ç–æ–∫–µ–Ω—ã –µ—Å—Ç—å –≤ –∏–Ω–¥–µ–∫—Å–µ: –ò (AND)
    - –µ—Å–ª–∏ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –ò–õ–ò (OR)
    """
    ensure_fresh_data()

    if not tokens:
        return set()

    normalized = [norm_code(t) for t in tokens if t]
    normalized = [t for t in normalized if t]

    if not normalized:
        return set()

    # AND phase
    sets = []
    for t in normalized:
        s = _search_index.get(t)
        if not s:
            sets = []
            break
        sets.append(s)

    if sets:
        result = sets[0].copy()
        for s in sets[1:]:
            result &= s
        return result

    # OR phase
    result = set()
    for t in normalized:
        result |= _search_index.get(t, set())

    return result


# ------------------------------------------------------------
#          FALLBACK SEARCH (—Å–ª–∞–±—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º)
# ------------------------------------------------------------

def search_fallback(df_: pd.DataFrame, q: str) -> List[int]:
    """
    –û—á–µ–Ω—å —Å–ª–∞–±—ã–π –ø–æ–∏—Å–∫:
    - –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ q_squash –≤ –∫–æ–¥, name, —Ç–∏–ø, oem
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ index match –Ω–∏—á–µ–≥–æ –Ω–µ –¥–∞–ª.
    """
    qsq = squash(q)
    if not qsq:
        return []

    out = []

    for i, row in df_.iterrows():
        code = squash(row.get("–∫–æ–¥", ""))
        name = squash(row.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", ""))
        typ = squash(row.get("—Ç–∏–ø", ""))
        oem = squash(row.get("oem", ""))

        combined = code + name + typ + oem
        if qsq in combined:
            out.append(i)

    return out


# ------------------------------------------------------------
#                        SCORING
# ------------------------------------------------------------

def relevance(row: dict, tokens: List[str], qsq: str) -> float:
    """
    –í–µ—Å–æ–≤–∞—è –º–æ–¥–µ–ª—å:
    - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –≤ –∫–æ–¥–µ ‚Üí 5
    - –≤ name ‚Üí 3
    - –≤ —Ç–∏–ø–µ ‚Üí 2
    - –≤ OEM ‚Üí 2
    - +10 –∑–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ squash
    - +100 –∑–∞ –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–¥–∞
    """
    tkns = [t.lower() for t in tokens if t.strip()]
    if not tkns:
        return 0.0

    code = str(row.get("–∫–æ–¥", "")).lower()
    name = str(row.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "")).lower()
    typ  = str(row.get("—Ç–∏–ø", "")).lower()
    oem  = str(row.get("oem", "")).lower()

    score = 0

    for t in tkns:
        if t in code: score += 5
        if t in name: score += 3
        if t in typ:  score += 2
        if t in oem:  score += 2

    # squash match
    joined = squash(code + name + typ + oem)
    if qsq and qsq in joined:
        score += 10

    # exact code match
    if norm_code(code) == norm_code(" ".join(tkns)):
        score += 100

    return score


# ------------------------------------------------------------
#               –û–ë–™–ï–î–ò–ù–Å–ù–ù–´–ô –ü–û–ò–°–ö (–≥–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)
# ------------------------------------------------------------

def search_rows(q: str) -> List[int]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –∫–∞–∫ –≤ –±–æ—Ç–µ:
    1) exact match
    2) index match (AND ‚Üí OR)
    3) fallback squash
    4) —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score
    """
    ensure_fresh_data()

    if not q.strip():
        return []

    tokens = q.split()
    qsq = squash(q)

    # 1) –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫
    exact = search_exact(df, q)
    if exact:
        return exact

    # 2) –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
    idx_hits = list(search_index(tokens))
    if idx_hits:
        # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ score
        return sorted(idx_hits, key=lambda i: -relevance(df.loc[i], tokens, qsq))

    # 3) fallback –ø–æ–∏—Å–∫
    fb = search_fallback(df, q)
    return fb
# ============================================================
#                       IMAGE RESOLUTION
# ============================================================

async def find_image_by_code_async(code: str) -> str:
    """
    –ò—â–µ—Ç —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–æ—Ç–æ —Å—Ç—Ä–æ–≥–æ –ø–æ –ö–û–î–£:
    1) –ü–æ image_index (–ø–æ—Å—Ç—Ä–æ–µ–Ω–æ –ø–æ filename tokens)
    2) –ü–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä image-—Å—Ç–æ–ª–±—Ü–∞, –∏—â–µ–º –∫–æ–¥ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞
    """
    ensure_fresh_data()

    if not code:
        return ""

    key = norm_code(code)
    if not key:
        return ""

    # 1) –∏–Ω–¥–µ–∫—Å
    url = _image_index.get(key)
    if url:
        return url

    # 2) fallback
    if df is not None and "image" in df.columns:
        for raw_url in df["image"]:
            u = str(raw_url or "").strip()
            if not u:
                continue

            tokens = url_name_tokens(u)
            joined = "".join(tokens)

            if key in tokens or key in joined:
                return u

    logger.info(f"[image] not found for code={key}")
    return ""


# -------------------------------------------------------------------
# Google Drive ‚Üí direct URL
# -------------------------------------------------------------------

def normalize_drive_url(url: str) -> str:
    m = re.search(
        r"drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))",
        str(url or "")
    )
    if not m:
        return url

    file_id = m.group(1) or m.group(2)
    return f"https://drive.google.com/uc?export=download&id={file_id}"


# -------------------------------------------------------------------
# iBB.co ‚Üí direct image resolver
# -------------------------------------------------------------------

async def resolve_ibb_direct_async(url: str) -> str:
    """
    –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ ibb.co ‚Äî –¥–æ—Å—Ç–∞—ë–º –ø—Ä—è–º–æ–π og:image URL.
    """
    try:
        if re.search(r"^https?://i\.ibb\.co/", url, re.I):
            return url

        if not re.search(r"^https?://ibb\.co/", url, re.I):
            return url

        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as r:
                if r.status != 200:
                    return url
                html = await r.text()

        m = re.search(
            r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)',
            html, re.I
        )
        return m.group(1) if m else url

    except Exception as e:
        logger.warning(f"resolve_ibb_direct_async error: {e}")
        return url


async def resolve_image_url_async(url_raw: str) -> str:
    if not url_raw:
        return ""

    url = normalize_drive_url(url_raw)
    url = await resolve_ibb_direct_async(url)
    return url


# ============================================================
#                        EXPORT
# ============================================================

def df_to_xlsx(df_: pd.DataFrame) -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
        df_.to_excel(writer, index=False)
    buf.seek(0)
    return buf


# ============================================================
#                  USERS / PERMISSIONS
# ============================================================

def parse_int(x) -> Optional[int]:
    try:
        v = int(str(x).strip())
        return v if v > 0 else None
    except Exception:
        return None


def normalize_header(h: str, idx: int) -> str:
    h = (h or "").strip().lower()
    h = re.sub(r"[^\w]+", "_", h).strip("_")
    return h or f"col{idx+1}"


def dedupe_headers(headers: List[str]) -> List[str]:
    out = []
    seen = {}
    for i, h in enumerate(headers):
        base = normalize_header(h, i)
        if base not in seen:
            seen[base] = 1
            out.append(base)
        else:
            seen[base] += 1
            out.append(f"{base}_{seen[base]}")
    return out


def load_users_from_sheet() -> Tuple[Set[int], Set[int], Set[int]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–∞:
    1) allowed
    2) admins
    3) blocked
    """
    allowed = set()
    admins = set()
    blocked = set()

    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(USERS_SHEET_NAME)
    except Exception:
        logger.info("No users sheet ‚Äî allow all")
        return allowed, admins, blocked

    vals = ws.get_all_values()
    if not vals:
        return allowed, admins, blocked

    headers = dedupe_headers(vals[0])
    rows = vals[1:]

    recs = []
    for r in rows:
        recs.append({
            headers[i]: (r[i] if i < len(r) else "")
            for i in range(len(headers))
        })

    d = pd.DataFrame(recs)
    d.columns = [c.strip().lower() for c in d.columns]

    def truth(v):
        s = str(v).strip().lower()
        return s in ("1", "yes", "–¥–∞", "true", "y")

    for _, r in d.iterrows():
        uid = parse_int(r.get("user_id") or r.get("uid") or r.get("id"))
        if not uid:
            continue

        # –†–æ–ª–µ–≤–æ–π —Ä–µ–∂–∏–º
        if "role" in d.columns:
            role = str(r.get("role", "")).strip().lower()
            if role in ("admin", "–∞–¥–º–∏–Ω"):
                admins.add(uid)
                allowed.add(uid)
            elif role in ("blocked", "ban", "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω"):
                blocked.add(uid)
            else:
                allowed.add(uid)
            continue

        # –ö–æ–ª–æ–Ω–∫–∏ allowed/admin/blocked
        if "blocked" in d.columns and truth(r.get("blocked")):
            blocked.add(uid)
            continue

        if "admin" in d.columns and truth(r.get("admin")):
            admins.add(uid)
            allowed.add(uid)
            continue

        if "allowed" in d.columns and truth(r.get("allowed")):
            allowed.add(uid)
            continue

        allowed.add(uid)

    return allowed, admins, blocked


# ============================================================
#                     INITIAL LOAD
# ============================================================

def initial_load():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–¥–ª—è —Å—Ç–∞—Ä—Ç–∞ –±–æ—Ç–∞)."""
    global SHEET_ALLOWED, SHEET_ADMINS, SHEET_BLOCKED

    try:
        ensure_fresh_data(force=True)
    except Exception as e:
        logger.exception("initial_load: failed to load sheet")
        raise

    try:
        allowed, admins, blocked = load_users_from_sheet()
        SHEET_ALLOWED = set(allowed)
        SHEET_ADMINS = set(admins)
        SHEET_BLOCKED = set(blocked)
    except Exception as e:
        logger.warning(f"initial_load: user sheet failed: {e}")


import asyncio

async def asyncio_to_thread(func, *args, **kwargs):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏."""
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(
        None, lambda: func(*args, **kwargs)
    )


async def initial_load_async():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ (–¥–ª—è web-app)."""
    global SHEET_ALLOWED, SHEET_ADMINS, SHEET_BLOCKED

    try:
        await asyncio_to_thread(ensure_fresh_data, True)
    except Exception as e:
        logger.exception("initial_load_async: failed to load sheet")
        raise

    try:
        allowed, admins, blocked = await asyncio_to_thread(
            load_users_from_sheet
        )
        SHEET_ALLOWED = set(allowed)
        SHEET_ADMINS = set(admins)
        SHEET_BLOCKED = set(blocked)

    except Exception as e:
        logger.warning(f"initial_load_async: user sheet failed: {e}")

