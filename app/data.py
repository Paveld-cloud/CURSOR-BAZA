# app/data.py
import os
import io
import re
import time
import json
import logging
from typing import Dict, Set, Tuple, List, Optional

import pandas as pd
import aiohttp
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from zoneinfo import ZoneInfo

logger = logging.getLogger("bot.data")

# ---------- –ö–æ–Ω—Ñ–∏–≥ ----------
# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –Ω–∞–∑–≤–∞–Ω–∏–π (—á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å —Å—Ç–∞—Ä—ã–µ ENV/–≤–µ—Ä—Å–∏–∏)
try:
    from app.config import (
        SPREADSHEET_URL,
        GOOGLE_APPLICATION_CREDENTIALS_JSON as _GA_CRED_FROM_CONFIG,
        TZ_NAME,
        DATA_TTL,
        USERS_TTL,
        IMAGE_STRICT,
        SAP_SHEET_NAME,
        USERS_SHEET_NAME,
        SEARCH_FIELDS,  # –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–º—è –≤ config.py
    )
    SEARCH_COLUMNS = SEARCH_FIELDS
    if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON"):
        # –µ—Å–ª–∏ ENV –Ω–µ –∑–∞–¥–∞–Ω, –±–µ—Ä—ë–º –∏–∑ config
        os.environ["GOOGLE_APPLICATION_CREDENTIALS_JSON"] = str(_GA_CRED_FROM_CONFIG or "")
except Exception:
    SPREADSHEET_URL = os.getenv("SPREADSHEET_URL", "")
    TZ_NAME = os.getenv("TIMEZONE", "Asia/Tashkent")
    DATA_TTL = int(os.getenv("DATA_TTL", "600"))
    USERS_TTL = int(os.getenv("USERS_TTL", "600"))
    IMAGE_STRICT = str(os.getenv("IMAGE_STRICT", "1")).strip().lower() in {"1", "true", "yes", "y", "–¥–∞", "ok", "–æ–∫"}
    SAP_SHEET_NAME = os.getenv("SAP_SHEET_NAME", "SAP")
    USERS_SHEET_NAME = os.getenv("USERS_SHEET_NAME", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    SEARCH_COLUMNS = [
        "—Ç–∏–ø",
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
        "–∫–æ–¥",
        "oem",
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å",
        "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
        "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
    ]

GOOGLE_APPLICATION_CREDENTIALS_JSON = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON", "")
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ---------- –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ----------
df: Optional[pd.DataFrame] = None
_last_load_ts: float = 0.0

_search_index: Dict[str, Set[int]] = {}
_image_index: Dict[str, str] = {}

# user/session states (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è handlers.py)
user_state: Dict[int, dict] = {}
issue_state: Dict[int, dict] = {}

SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()

_last_users_load_ts: float = 0.0

ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)


# ---------- –£—Ç–∏–ª–∏—Ç—ã ----------
def _norm_code(x: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–æ–≤:
    - lower
    - –∑–∞–º–µ–Ω–∏—Ç—å –±—É–∫–≤—É 'o' –Ω–∞ —Ü–∏—Ñ—Ä—É '0'
    - —É–±—Ä–∞—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∫—Ä–æ–º–µ [a-z0-9]
    """
    s = str(x or "").strip().lower()
    s = s.replace("o", "0")  # –±—É–∫–≤–∞ O ‚Üí —Ü–∏—Ñ—Ä–∞ 0
    s = re.sub(r"[^a-z0-9]", "", s)
    return s


def _norm_str(x: str) -> str:
    return str(x or "").strip().lower()


def now_local_str(tz_name: str = None) -> str:
    tz = ZoneInfo(tz_name or TZ_NAME or "Asia/Tashkent")
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")


def val(d: dict, key: str, default: str = "") -> str:
    return str(d.get(key, default) or default)


def _url_name_tokens(url: str) -> List[str]:
    """
    –¢–æ–∫–µ–Ω—ã –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞/–ø—É—Ç–∏ URL:
    - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ latin+digits
    - –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏–∑ URL –æ–±—ã—á–Ω–æ –Ω–µ –≤—Å—Ç—Ä–µ—Ç–∏—à—å, –Ω–æ –Ω–µ –ª–æ–º–∞–µ–º—Å—è
    """
    try:
        path = re.sub(r"[?#].*$", "", str(url or ""))
        name = path.rsplit("/", 1)[-1].rsplit(".", 1)[0].lower()
        return re.findall(r"[0-9a-z–∞-—è—ë]+", name, flags=re.I)
    except Exception:
        return []


def squash(text: str) -> str:
    return re.sub(r"[\W_]+", "", str(text or "").lower(), flags=re.U)


def normalize(text: str) -> str:
    return re.sub(r"[^\w\s]", "", str(text or "").lower(), flags=re.U).strip()


def _normalize_header_name(h: str, idx: int) -> str:
    name = (h or "").strip().lower()
    name = re.sub(r"[^\w]+", "_", name, flags=re.U).strip("_")
    if not name:
        name = f"col{idx+1}"
    return name


def _dedupe_headers(headers: List[str]) -> List[str]:
    seen: Dict[str, int] = {}
    out: List[str] = []
    for i, h in enumerate(headers):
        base = _normalize_header_name(h, i)
        if base not in seen:
            seen[base] = 1
            out.append(base)
        else:
            seen[base] += 1
            out.append(f"{base}_{seen[base]}")
    return out


# ---------- –§–æ—Ä–º–∞—Ç –∫–∞—Ä—Ç–æ—á–∫–∏ ----------
def format_row(row: dict) -> str:
    """
    –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞: –ö–û–î –ø–µ—Ä–≤—ã–º, –∑–∞—Ç–µ–º –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ, –¢–∏–ø –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è.
    –ü–ª–æ—Ç–Ω–∞—è –≤–µ—Ä—Å—Ç–∫–∞ –ø–æ–¥ –º–æ–±–∏–ª—å–Ω—ã–π Telegram (HTML).
    """
    code        = val(row, "–∫–æ–¥").upper()
    name        = val(row, "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ")
    type_       = val(row, "—Ç–∏–ø")
    part_no     = val(row, "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä")
    oem_part    = val(row, "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä")
    qty         = val(row, "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ") or "‚Äî"
    price       = val(row, "—Ü–µ–Ω–∞")
    currency    = val(row, "–≤–∞–ª—é—Ç–∞")
    manuf       = val(row, "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å")
    oem         = val(row, "oem")

    lines: List[str] = []

    if code:
        lines.append(f"üî¢ <b>–ö–æ–¥:</b> {code}")
    if name:
        lines.append(f"üì¶ <b>–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ:</b> {name}")
    if type_:
        lines.append(f"üìé <b>–¢–∏–ø:</b> {type_}")
    if part_no:
        lines.append(f"üß© <b>–ü–∞—Ä—Ç ‚Ññ:</b> {part_no}")
    if oem_part:
        lines.append(f"‚öôÔ∏è <b>OEM ‚Ññ:</b> {oem_part}")

    lines.append(f"üì¶ <b>–ö–æ–ª-–≤–æ:</b> {qty}")

    if price or currency:
        lines.append(f"üí∞ <b>–¶–µ–Ω–∞:</b> {price} {currency}".rstrip())

    if manuf:
        lines.append(f"üè≠ <b>–ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å:</b> {manuf}")
    if oem:
        lines.append(f"‚öôÔ∏è <b>OEM:</b> {oem}")

    return "\n".join(lines)


# ---------- Google Sheets ----------
def get_gs_client():
    if not GOOGLE_APPLICATION_CREDENTIALS_JSON:
        raise RuntimeError("GOOGLE_APPLICATION_CREDENTIALS_JSON –Ω–µ –∑–∞–¥–∞–Ω")
    try:
        info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
        creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    except json.JSONDecodeError:
        creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS_JSON, scopes=SCOPES)
    return gspread.authorize(creds)


def _load_sap_dataframe() -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫, –∫–∞–∫ –æ–Ω–∏ –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –≤ Google Sheets (get_all_values).
    –ü–ª—é—Å:
    - –¥–µ–¥—É–ø –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–µ—Å–ª–∏ –≤ –ª–∏—Å—Ç–µ –¥—É–±–ª–∏ –∫–æ–ª–æ–Ω–æ–∫)
    - –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–æ–∫ (strip), –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ü–µ–Ω—ã/—á–∏—Å–µ–ª (–∫–∞–∫ –≤ Sheets)
    """
    if not SPREADSHEET_URL:
        raise RuntimeError("SPREADSHEET_URL –Ω–µ –∑–∞–¥–∞–Ω")

    client = get_gs_client()
    sh = client.open_by_url(SPREADSHEET_URL)
    ws = sh.worksheet(SAP_SHEET_NAME)

    values = ws.get_all_values()
    if not values:
        return pd.DataFrame()

    headers_raw = values[0]
    headers = _dedupe_headers([c.strip() for c in headers_raw])
    rows = values[1:]

    new_df = pd.DataFrame(rows, columns=headers)

    # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å—ë –∫ —Å—Ç—Ä–æ–∫–µ –∏ —á–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã (—á—Ç–æ–±—ã –ø–æ–∏—Å–∫/–∏–Ω–¥–µ–∫—Å –±—ã–ª–∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ)
    for c in new_df.columns:
        new_df[c] = new_df[c].astype(str).fillna("").map(lambda x: str(x).strip())

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ code-like –ø–æ–ª—è –¥–ª—è –ø–æ–∏—Å–∫–∞
    for col in ("–∫–æ–¥", "oem", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).map(lambda x: str(x).strip().lower())

    # image ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∫—É (URL)
    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).map(lambda x: str(x).strip())

    return new_df


# ---------- –ò–Ω–¥–µ–∫—Å—ã ----------
def build_search_index(df_: pd.DataFrame) -> Dict[str, Set[int]]:
    """
    –ò–Ω–¥–µ–∫—Å:
    - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã + –ª–∞—Ç–∏–Ω–∏—Ü—ã + —Ü–∏—Ñ—Ä
    - –¥–ª—è –∫–æ–¥–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º norm_code (O->0, —á–∏—Å—Ç–∫–∞)
    """
    idx: Dict[str, Set[int]] = {}
    cols = [c for c in SEARCH_COLUMNS if c in df_.columns]

    token_re = re.compile(r"[0-9a-z–∞-—è—ë]+", flags=re.I)

    for i, row in df_.iterrows():
        for c in cols:
            raw = str(row.get(c, "") or "").strip().lower()
            if not raw:
                continue

            # code-like –∫–ª—é—á–∏
            if c in ("–∫–æ–¥", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem"):
                norm = _norm_code(raw)
                if norm:
                    idx.setdefault(norm, set()).add(i)

            # —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã (–≤–∫–ª—é—á–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É)
            for t in token_re.findall(raw):
                key = _norm_str(t)
                if key and len(key) >= 2:
                    idx.setdefault(key, set()).add(i)

    return idx


def build_image_index(df_: pd.DataFrame) -> Dict[str, str]:
    """
    –ò–Ω–¥–µ–∫—Å –∫–∞—Ä—Ç–∏–Ω–æ–∫ –ø–æ —Ç–æ–∫–µ–Ω–∞–º –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.
    –ö–ª—é—á–∏ —Ö—Ä–∞–Ω–∏–º –≤ norm_code(), —á—Ç–æ–±—ã –∫–æ–¥—ã —Ç–∏–ø–∞ 225177765, 3202-b-2rsr, i28 –∏ —Ç.–¥. –º–∞—Ç—á–∏–ª–∏—Å—å —Å—Ç–∞–±–∏–ª—å–Ω–æ.
    """
    index: Dict[str, str] = {}
    if "image" not in df_.columns:
        return index

    skip = {"png", "jpg", "jpeg", "gif", "webp", "svg"}

    for _, row in df_.iterrows():
        url = str(row.get("image", "")).strip()
        if not url:
            continue

        tokens = _url_name_tokens(url)
        # –∫–ª—é—á–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        for t in tokens:
            if t in skip or len(t) < 3:
                continue
            index.setdefault(_norm_code(t), url)

        # –¥–æ–ø. –∫–ª—é—á: —Å–ª–µ–ø–ª–µ–Ω–Ω–æ–µ –∏–º—è
        join = "".join(tokens)
        if join:
            index.setdefault(_norm_code(join), url)

    return index


def ensure_fresh_data(force: bool = False):
    global df, _search_index, _image_index, _last_load_ts
    need = force or df is None or (time.time() - _last_load_ts > DATA_TTL)
    if not need:
        return

    new_df = _load_sap_dataframe()
    df = new_df
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()
    logger.info(f"‚úÖ SAP reload: {len(df)} rows, index={len(_search_index)} keys, images={len(_image_index)} keys")


# ---------- –ö–∞—Ä—Ç–∏–Ω–∫–∏ ----------
async def find_image_by_code_async(code: str) -> str:
    """
    IMAGE_STRICT=1:
      - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º URL —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–æ–¥ —Ä–µ–∞–ª—å–Ω–æ ‚Äú–≤ –∏–º–µ–Ω–∏‚Äù (—á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å/—Ç–æ–∫–µ–Ω—ã)
    IMAGE_STRICT=0:
      - –º–æ–∂–Ω–æ –≤–µ—Ä–Ω—É—Ç—å image –∏–∑ —Å—Ç—Ä–æ–∫–∏ –Ω–∞–ø—Ä—è–º—É—é (–µ—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ –ª–µ–∂–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL –ø–æ–¥ –¥–µ—Ç–∞–ª—å)
    """
    ensure_fresh_data()
    if not code:
        return ""

    key = _norm_code(code)
    if not key:
        return ""

    # 1) –∏–Ω–¥–µ–∫—Å –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
    hit = _image_index.get(key)
    if hit:
        return hit

    # 2) —Å—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º ‚Äî –Ω–∞ —ç—Ç–æ–º —Å—Ç–æ–ø
    if IMAGE_STRICT:
        logger.info(f"[image][strict] –Ω–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è –∫–æ–¥–∞: {key}")
        return ""

    # 3) –º—è–≥–∫–∏–π —Ñ–æ–ª–±—ç–∫: –ø–æ–ø—ã—Ç–∞—Ç—å—Å—è –Ω–∞–π—Ç–∏ URL –≤ —Å—Ç—Ä–æ–∫–µ, –≥–¥–µ "–∫–æ–¥" —Å–æ–≤–ø–∞–¥–∞–µ—Ç
    try:
        if df is not None and "–∫–æ–¥" in df.columns and "image" in df.columns:
            # —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –∫–æ–¥—É –≤ —Å—Ç—Ä–æ–∫–µ
            for _, r in df.iterrows():
                if _norm_code(r.get("–∫–æ–¥", "")) == key:
                    u = str(r.get("image", "")).strip()
                    return u or ""
    except Exception as e:
        logger.warning(f"find_image_by_code_async soft fallback error: {e}")

    return ""


def normalize_drive_url(url: str) -> str:
    m = re.search(r"drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))", str(url or ""))
    if m:
        file_id = m.group(1) or m.group(2)
        return f"https://drive.google.com/uc?export=download&id={file_id}"
    return str(url or "")


async def resolve_ibb_direct_async(url: str) -> str:
    try:
        if re.search(r"^https?://i\.ibb\.co/", url, re.I):
            return url
        if not re.search(r"^https?://ibb\.co/", url, re.I):
            return url

        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as resp:
                if resp.status != 200:
                    return url
                html = await resp.text()

        m = re.search(
            r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
            html,
            re.I
        )
        return m.group(1) if m else url
    except Exception as e:
        logger.warning(f"resolve_ibb_direct_async error: {e}")
        return url


async def resolve_image_url_async(url_raw: str) -> str:
    if not url_raw:
        return ""
    url = normalize_drive_url(url_raw)
    url = await resolve_ibb_direct_async(url)
    return url


# ---------- –ü–æ–∏—Å–∫ ----------
def _token_keys(raw_token: str) -> List[str]:
    """
    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ —Å—Ç—Ä–æ–∏–º –∫–ª—é—á–∏:
    - —Ç–µ–∫—Å—Ç–æ–≤—ã–π (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞/–ª–∞—Ç–∏–Ω–∏—Ü–∞)
    - –∫–æ–¥–æ–≤—ã–π (–¥–ª—è part/code/oem)
    """
    t = str(raw_token or "").strip().lower()
    if not t:
        return []
    keys = []
    s = _norm_str(t)
    if s:
        keys.append(s)
    c = _norm_code(t)
    if c and c != s:
        keys.append(c)
    return keys


def match_row_by_index(tokens: List[str]) -> Set[int]:
    """
    –õ–æ–≥–∏–∫–∞:
    - –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –±–µ—Ä—ë–º UNION –ø–æ (str_key, code_key)
    - –∑–∞—Ç–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç–æ–∫–µ–Ω–∞–º (AND)
    - –µ—Å–ª–∏ AND –ø—É—Å—Ç–æ–π ‚Äî –æ—Å–ª–∞–±–ª—è–µ–º –¥–æ OR
    """
    ensure_fresh_data()
    if not tokens:
        return set()

    per_token_sets: List[Set[int]] = []
    for t in tokens:
        keys = _token_keys(t)
        if not keys:
            continue
        u: Set[int] = set()
        for k in keys:
            u |= _search_index.get(k, set())
        if u:
            per_token_sets.append(u)
        else:
            # –µ—Å–ª–∏ —Ç–æ–∫–µ–Ω –≤–æ–æ–±—â–µ –Ω–∏–≥–¥–µ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî AND —É–∂–µ –Ω–µ —Å–ª–æ–∂–∏—Ç—Å—è
            per_token_sets.append(set())

    if not per_token_sets:
        return set()

    # AND
    acc = per_token_sets[0].copy()
    for s in per_token_sets[1:]:
        acc &= s

    if acc:
        return acc

    # OR
    found: Set[int] = set()
    for s in per_token_sets:
        found |= s
    return found


def _relevance_score(row: dict, tokens: List[str], q_squash: str) -> float:
    tkns = [_norm_str(t) for t in tokens if t]
    if not tkns:
        return 0.0

    code = _norm_str(row.get("–∫–æ–¥", ""))
    name = _norm_str(row.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", ""))
    ttype = _norm_str(row.get("—Ç–∏–ø", ""))
    oem  = _norm_str(row.get("oem", ""))
    manuf = _norm_str(row.get("–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å", ""))

    weights = {
        "–∫–æ–¥": 5.0,
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": 3.0,
        "—Ç–∏–ø": 2.0,
        "oem": 2.0,
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": 2.0,
    }
    fields = {
        "–∫–æ–¥": code,
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
        "—Ç–∏–ø": ttype,
        "oem": oem,
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": manuf,
    }

    score = 0.0
    for f, text in fields.items():
        for t in tkns:
            if t and (t in text):
                score += weights[f]

    if q_squash:
        joined = squash(code + name + ttype + oem + manuf)
        if q_squash in joined:
            score += 10.0

    q_full = " ".join(tkns)
    q_full_no_ws = squash(q_full)
    if code:
        if code == q_full:
            score += 100.0
        if code.startswith(q_full) or code.startswith(q_full_no_ws):
            score += 20.0
        for t in tkns:
            if code.startswith(t):
                score += 5.0

    return score


# ---------- –≠–∫—Å–ø–æ—Ä—Ç ----------
def _df_to_xlsx(df_: pd.DataFrame, filename: str = "export.xlsx") -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
        df_.to_excel(writer, index=False)
    buf.seek(0)
    return buf


# ---------- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ ----------
def _parse_int(x) -> Optional[int]:
    try:
        v = int(str(x).strip())
        return v if v > 0 else None
    except Exception:
        return None


def load_users_from_sheet() -> Tuple[Set[int], Set[int], Set[int]]:
    """
    –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ö–µ–º:
    - role=admin/blocked/...
    - allowed/admin/blocked=1/–¥–∞/true
    """
    allowed: Set[int] = set()
    admins: Set[int] = set()
    blocked: Set[int] = set()

    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(USERS_SHEET_NAME)
    except Exception:
        logger.info("–õ–∏—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –ø—É—Å–∫–∞–µ–º –≤—Å–µ—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        return allowed, admins, blocked

    all_vals = ws.get_all_values()
    if not all_vals:
        return allowed, admins, blocked

    headers_raw = all_vals[0]
    headers = _dedupe_headers(headers_raw)
    rows = all_vals[1:]

    recs: List[dict] = []
    for r in rows:
        recs.append({headers[i]: (r[i] if i < len(r) else "") for i in range(len(headers))})

    dfu = pd.DataFrame(recs)
    dfu.columns = [c.strip().lower() for c in dfu.columns]

    has_role = "role" in dfu.columns
    has_allowed = "allowed" in dfu.columns
    has_admin = "admin" in dfu.columns
    has_blocked = "blocked" in dfu.columns

    def truthy(v) -> bool:
        s = str(v).strip().lower()
        return s in ("1", "true", "–¥–∞", "y", "yes")

    for _, r in dfu.iterrows():
        uid = _parse_int(r.get("user_id") or r.get("uid") or r.get("id"))
        if not uid:
            continue

        if has_role:
            role = str(r.get("role", "")).strip().lower()
            if role in ("admin", "–∞–¥–º–∏–Ω"):
                admins.add(uid)
                allowed.add(uid)
            elif role in ("blocked", "ban", "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω"):
                blocked.add(uid)
            else:
                allowed.add(uid)
            continue

        if has_blocked and truthy(r.get("blocked")):
            blocked.add(uid)
            continue
        if has_admin and truthy(r.get("admin")):
            admins.add(uid)
            allowed.add(uid)
            continue
        if has_allowed and truthy(r.get("allowed")):
            allowed.add(uid)
            continue

        allowed.add(uid)

    return allowed, admins, blocked


def ensure_fresh_users(force: bool = False):
    """
    –ö—ç—à–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ USERS_TTL, —á—Ç–æ–±—ã –Ω–µ –¥–æ–ª–±–∏—Ç—å Sheets –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å.
    """
    global _last_users_load_ts
    need = force or (time.time() - _last_users_load_ts > USERS_TTL) or (not SHEET_ALLOWED and not SHEET_ADMINS and not SHEET_BLOCKED)
    if not need:
        return

    try:
        allowed, admins, blocked = load_users_from_sheet()
        SHEET_ALLOWED.clear(); SHEET_ALLOWED.update(allowed)
        SHEET_ADMINS.clear(); SHEET_ADMINS.update(admins)
        SHEET_BLOCKED.clear(); SHEET_BLOCKED.update(blocked)
        _last_users_load_ts = time.time()
        logger.info(f"‚úÖ USERS reload: allowed={len(SHEET_ALLOWED)} admins={len(SHEET_ADMINS)} blocked={len(SHEET_BLOCKED)}")
    except Exception as e:
        logger.warning(f"ensure_fresh_users error: {e}")


# ---------- Async helper ----------
import asyncio
async def asyncio_to_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, lambda: func(*args, **kwargs))


# ---------- Backward-compat ----------
def initial_load():
    ensure_fresh_data(force=True)
    ensure_fresh_users(force=True)


async def initial_load_async():
    await asyncio_to_thread(ensure_fresh_data, True)
    await asyncio_to_thread(ensure_fresh_users, True)
