import os
import io
import re
import time
import json
import logging
from typing import Dict, Set, Tuple, List, Optional

import pandas as pd
import aiohttp
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from zoneinfo import ZoneInfo

logger = logging.getLogger("bot.data")

# ---------- –ö–æ–Ω—Ñ–∏–≥ ----------
try:
    from app.config import (
        SPREADSHEET_URL,
        SAP_SHEET_NAME,
        USERS_SHEET_NAME,
        DATA_TTL,
        SEARCH_COLUMNS,
    )
except Exception:
    SPREADSHEET_URL = os.getenv("SPREADSHEET_URL", "")
    SAP_SHEET_NAME = os.getenv("SAP_SHEET_NAME", "SAP")
    USERS_SHEET_NAME = os.getenv("USERS_SHEET_NAME", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    DATA_TTL = int(os.getenv("DATA_TTL", "600"))
    SEARCH_COLUMNS = [
        "—Ç–∏–ø",
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
        "–∫–æ–¥",
        "oem",
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å",
        "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
        "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
    ]

GOOGLE_APPLICATION_CREDENTIALS_JSON = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON", "")
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ---------- –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ----------
df: Optional[pd.DataFrame] = None
_last_load_ts: float = 0.0
_search_index: Dict[str, Set[int]] = {}
_image_index: Dict[str, str] = {}

user_state: Dict[int, dict] = {}
issue_state: Dict[int, dict] = {}

ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# ---------- –£—Ç–∏–ª–∏—Ç—ã ----------
def normalize(text: str) -> str:
    """–ö–∞–∫ –≤ –±–æ—Ç–µ ‚Äî –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ–∏—Å–∫–∞."""
    return re.sub(r"[^\w\s]", "", str(text or "").lower()).strip()

def squash(text: str) -> str:
    """–£–ø—Ä–æ—â–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –Ω–µ—á—ë—Ç–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞."""
    return re.sub(r"[\W_]+", "", str(text or "").lower())

def norm_code(val: str) -> str:
    """–ü–æ–ª–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–∞ ‚Äî –∫–∞–∫ –≤ –±–æ—Ç–µ."""
    s = str(val or "").strip().lower()
    s = s.replace("o", "0")
    return re.sub(r"[^a-z0-9]", "", s)

# —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
_norm_code = norm_code

def _url_name_tokens(url: str) -> List[str]:
    try:
        name = str(url).split("/")[-1].split(".")[0].lower()
        return re.findall(r"[a-z0-9]+", name)
    except:
        return []

def _safe_col(df_: pd.DataFrame, col: str) -> Optional[pd.Series]:
    if col not in df_.columns:
        return None
    return df_[col].astype(str).fillna("").str.strip().str.lower()

def val(d: dict, key: str, default: str = "") -> str:
    return str(d.get(key, default) or default)

def now_local_str(tz_name: str = "Asia/Tashkent") -> str:
    tz = ZoneInfo(tz_name)
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")
# ---------- Google Sheets ----------
def get_gs_client():
    if not GOOGLE_APPLICATION_CREDENTIALS_JSON:
        raise RuntimeError("GOOGLE_APPLICATION_CREDENTIALS_JSON –Ω–µ –∑–∞–¥–∞–Ω")

    try:
        info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
        creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    except json.JSONDecodeError:
        creds = Credentials.from_service_account_file(
            GOOGLE_APPLICATION_CREDENTIALS_JSON,
            scopes=SCOPES
        )
    return gspread.authorize(creds)


def _load_sap_dataframe() -> pd.DataFrame:
    """–ó–∞–≥—Ä—É–∑–∫–∞ SAP-–ª–∏—Å—Ç–∞ 1:1 –∫–∞–∫ –≤ –±–æ—Ç–µ ‚Äî –±–µ–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö."""
    client = get_gs_client()
    sh = client.open_by_url(SPREADSHEET_URL)
    ws = sh.worksheet(SAP_SHEET_NAME)

    values = ws.get_all_values()
    if not values:
        return pd.DataFrame()

    headers = [c.strip().lower() for c in values[0]]
    rows = values[1:]

    df_ = pd.DataFrame(rows, columns=headers)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–æ–≤, OEM –∏ –ø–∞—Ä—Ç-–Ω–æ–º–µ—Ä–æ–≤ ‚Äî –∫–∞–∫ –≤ –±–æ—Ç–µ
    for col in ("–∫–æ–¥", "oem", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"):
        if col in df_.columns:
            df_[col] = df_[col].astype(str).str.strip().str.lower()

    if "image" in df_.columns:
        df_["image"] = df_["image"].astype(str).str.strip()

    return df_


# ---------- –ò–Ω–¥–µ–∫—Å—ã ----------
def build_search_index(df_: pd.DataFrame) -> Dict[str, Set[int]]:
    """1:1 –∫–∞–∫ –≤ Telegram –±–æ—Ç–µ ‚Äî —Ç–æ–∫–µ–Ω—ã + –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–æ–≤."""
    idx: Dict[str, Set[int]] = {}
    cols = [c for c in SEARCH_COLUMNS if c in df_.columns]

    for i, row in df_.iterrows():
        for c in cols:
            val_ = str(row.get(c, "")).lower()

            # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ / –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä / OEM –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä
            if c in ("–∫–æ–¥", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"):
                nc = norm_code(val_)
                if nc:
                    idx.setdefault(nc, set()).add(i)

            # —Ç–æ–∫–µ–Ω—ã a-z0-9
            for t in re.findall(r"[a-z0-9]+", val_):
                t = t.strip().lower()
                if t:
                    idx.setdefault(t, set()).add(i)

    return idx


def build_image_index(df_: pd.DataFrame) -> Dict[str, str]:
    """–ò–Ω–¥–µ–∫—Å –∫–∞—Ä—Ç–∏–Ω–æ–∫ ‚Äî –∫–∞–∫ –≤ –±–æ—Ç–µ. –°–∫–∞–Ω–∏—Ä—É—é—Ç—Å—è —Ç–æ–∫–µ–Ω—ã –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞."""
    index: Dict[str, str] = {}
    if "image" not in df_.columns:
        return index

    skip = {"png", "jpg", "jpeg", "gif", "webp", "svg"}

    for _, row in df_.iterrows():
        url = str(row.get("image", "")).strip()
        if not url:
            continue

        tokens = _url_name_tokens(url)
        for t in tokens:
            if t in skip or len(t) < 3:
                continue
            index.setdefault(norm_code(t), url)

        # –∫–ª—é—á –Ω–∞ —Å–∫–ª–µ—ë–Ω–Ω–æ–º –∏–º–µ–Ω–∏
        index.setdefault("".join(tokens), url)

    return index


def ensure_fresh_data(force: bool = False):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–Ω–¥–µ–∫—Å–æ–≤ (–ø–æ –ª–æ–≥–∏–∫–µ Telegram-–±–æ—Ç–∞)."""
    global df, _search_index, _image_index, _last_load_ts

    need = (
        force
        or df is None
        or (time.time() - _last_load_ts > DATA_TTL)
    )
    if not need:
        return

    logger.info("üì• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ SAP-–¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Sheets...")

    df = _load_sap_dataframe()
    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _last_load_ts = time.time()

    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫, –∏–Ω–¥–µ–∫—Å–æ–≤: search={len(_search_index)}, images={len(_image_index)}")


# ---------- –ü–æ–∏—Å–∫ ----------
def match_row_by_index(tokens: List[str]) -> Set[int]:
    """–¢–æ—á–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º Telegram-–±–æ—Ç–∞ ‚Äî AND ‚Üí OR fallback."""
    ensure_fresh_data()

    if not tokens:
        return set()

    tokens_norm = [norm_code(t) for t in tokens if t]
    if not tokens_norm:
        return set()

    sets: List[Set[int]] = []

    # AND
    for t in tokens_norm:
        s = _search_index.get(t, set())
        if not s:
            sets = []
            break
        sets.append(s)

    if sets:
        acc = sets[0].copy()
        for s in sets[1:]:
            acc &= s
        return acc

    # OR fallback
    found: Set[int] = set()
    for t in tokens_norm:
        found |= _search_index.get(t, set())

    return found


def relevance_score(row: dict, tokens: List[str], q_squash: str) -> float:
    """–¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ 1:1."""
    tkns = [t.lower() for t in tokens if t]
    if not tkns:
        return 0.0

    code = row.get("–∫–æ–¥", "").lower()
    name = row.get("–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ", "").lower()
    type_ = row.get("—Ç–∏–ø", "").lower()
    oem = row.get("oem", "").lower()
    manuf = row.get("–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å", "").lower()

    weights = {
        "–∫–æ–¥": 5.0,
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": 3.0,
        "—Ç–∏–ø": 2.0,
        "oem": 2.0,
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": 2.0,
    }
    fields = {
        "–∫–æ–¥": code,
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ": name,
        "—Ç–∏–ø": type_,
        "oem": oem,
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å": manuf,
    }

    score = 0.0

    # —Ç–æ–∫–µ–Ω—ã
    for f, text in fields.items():
        for t in tkns:
            if t in text:
                score += weights[f]

    # squash
    if q_squash:
        joined = squash(code + name + type_ + oem + manuf)
        if q_squash in joined:
            score += 10.0

    # —Å–∏–ª—å–Ω—ã–π –±—É—Å—Ç –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–¥–∞
    if code == " ".join(tkns):
        score += 100.0

    for t in tkns:
        if code.startswith(t):
            score += 20.0

    return score


# ---------- –ö–∞—Ä—Ç–∏–Ω–∫–∏ ----------
async def resolve_ibb_direct_async(url: str) -> str:
    try:
        if url.startswith("https://i.ibb.co/"):
            return url
        if not url.startswith("https://ibb.co/"):
            return url

        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as resp:
                if resp.status != 200:
                    return url
                html = await resp.text()

        m = re.search(r'property="og:image" content="([^"]+)"', html)
        return m.group(1) if m else url

    except Exception:
        return url


async def resolve_image_url_async(url_raw: str) -> str:
    if not url_raw:
        return ""

    # google drive
    m = re.search(r"drive\.google\.com/(?:file/d/([-\w]{10,})|open\?id=([-\w]{10,}))", url_raw)
    if m:
        fid = m.group(1) or m.group(2)
        return f"https://drive.google.com/uc?export=download&id={fid}"

    # ibb.co
    return await resolve_ibb_direct_async(url_raw)


async def find_image_by_code_async(code: str) -> str:
    ensure_fresh_data()

    if not code:
        return ""

    key = norm_code(code)
    hit = _image_index.get(key)
    if hit:
        return hit

    # fallback ‚Äî –ø–æ–ª–Ω—ã–π –ø–µ—Ä–µ–±–æ—Ä
    try:
        if df is not None and "image" in df.columns:
            for url in df["image"]:
                url = str(url or "").strip()
                if not url:
                    continue

                tokens = _url_name_tokens(url)
                joined = "".join(tokens)

                if key in tokens or key in joined:
                    return url

    except Exception:
        pass

    return ""
# ---------- –≠–∫—Å–ø–æ—Ä—Ç ----------
def df_to_xlsx(df_: pd.DataFrame, filename: str = "export.xlsx") -> io.BytesIO:
    buf = io.BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
        df_.to_excel(writer, index=False)
    buf.seek(0)
    return buf


# ---------- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ ----------
def _parse_int(x) -> Optional[int]:
    try:
        n = int(str(x).strip())
        return n if n > 0 else None
    except Exception:
        return None


def _normalize_header_name(h: str, idx: int) -> str:
    name = (h or "").strip().lower()
    name = re.sub(r"[^\w]+", "_", name).strip("_")
    if not name:
        name = f"col{idx}"
    return name


def _dedupe_headers(headers: List[str]) -> List[str]:
    seen = {}
    out = []
    for i, h in enumerate(headers):
        base = _normalize_header_name(h, i)
        if base not in seen:
            seen[base] = 1
            out.append(base)
        else:
            seen[base] += 1
            out.append(f"{base}_{seen[base]}")
    return out


def load_users_from_sheet() -> Tuple[Set[int], Set[int], Set[int]]:
    """
    –õ–æ–≥–∏–∫–∞ 1:1 –∫–∞–∫ –≤ Telegram-–±–æ—Ç–µ.
    –ï—Å–ª–∏ –Ω–µ—Ç –ª–∏—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∏ —Ä–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ–º.
    """
    allowed = set()
    admins = set()
    blocked = set()

    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(USERS_SHEET_NAME)
    except Exception:
        logger.info("No users sheet ‚Äî allow all")
        return allowed, admins, blocked

    vals = ws.get_all_values()
    if not vals:
        return allowed, admins, blocked

    headers = _dedupe_headers(vals[0])
    rows = vals[1:]

    records = []
    for r in rows:
        rec = {headers[i]: (r[i] if i < len(r) else "") for i in range(len(headers))}
        records.append(rec)

    dfu = pd.DataFrame(records)
    dfu.columns = [c.strip().lower() for c in dfu.columns]

    has_role = "role" in dfu.columns
    has_allowed = "allowed" in dfu.columns
    has_admin = "admin" in dfu.columns
    has_blocked = "blocked" in dfu.columns

    def truthy(v) -> bool:
        return str(v).strip().lower() in ("1", "true", "–¥–∞", "yes", "y")

    for _, r in dfu.iterrows():
        uid = _parse_int(r.get("user_id") or r.get("uid") or r.get("id"))
        if not uid:
            continue

        if has_role:
            role = str(r.get("role", "")).strip().lower()
            if role in ("admin", "–∞–¥–º–∏–Ω"):
                admins.add(uid)
                allowed.add(uid)
            elif role in ("blocked", "ban", "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω"):
                blocked.add(uid)
            else:
                allowed.add(uid)
            continue

        if has_blocked and truthy(r.get("blocked")):
            blocked.add(uid)
            continue

        if has_admin and truthy(r.get("admin")):
            admins.add(uid)
            allowed.add(uid)
            continue

        if has_allowed and truthy(r.get("allowed")):
            allowed.add(uid)
            continue

        allowed.add(uid)

    return allowed, admins, blocked


# ---------- Async helper ----------
import asyncio

async def asyncio_to_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(
        None,
        lambda: func(*args, **kwargs)
    )


# ---------- INITIAL LOAD (–±–æ—Ç & mini app) ----------
def initial_load():
    """
    –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Telegram-–±–æ—Ç–µ –∏–ª–∏ —Ç–µ—Å—Ç–∞—Ö.
    """
    try:
        ensure_fresh_data(force=True)
    except Exception as e:
        logger.exception(f"initial_load failed: {e}")
        raise

    try:
        allowed, admins, blocked = load_users_from_sheet()
        SHEET_ALLOWED.clear();   SHEET_ALLOWED.update(allowed)
        SHEET_ADMINS.clear();    SHEET_ADMINS.update(admins)
        SHEET_BLOCKED.clear();   SHEET_BLOCKED.update(blocked)
    except Exception as e:
        logger.warning(f"initial_load: cannot load users: {e}")


async def initial_load_async():
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ mini app (aiohttp + FastAPI + Flask async).
    """
    try:
        await asyncio_to_thread(ensure_fresh_data, True)
    except Exception as e:
        logger.exception(f"initial_load_async: data load failed: {e}")
        raise

    try:
        allowed, admins, blocked = await asyncio_to_thread(load_users_from_sheet)
        SHEET_ALLOWED.clear();   SHEET_ALLOWED.update(allowed)
        SHEET_ADMINS.clear();    SHEET_ADMINS.update(admins)
        SHEET_BLOCKED.clear();   SHEET_BLOCKED.update(blocked)
    except Exception as e:
        logger.warning(f"initial_load_async: cannot load users: {e}")
