from pathlib import Path

data_py = r'''# app/data.py
import os
import io
import re
import time
import json
import logging
from typing import Dict, Set, List, Optional, Tuple

import pandas as pd
import aiohttp
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
from zoneinfo import ZoneInfo
from urllib.parse import urlparse
import os.path

logger = logging.getLogger("bot.data")

# ---------------- Config ----------------
# –ë–µ—Ä—ë–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ app/config.py, –Ω–æ —Å —Ñ–æ–ª–±—ç–∫–∞–º–∏ –Ω–∞ env.
# –í–ê–ñ–ù–û: –Ω–µ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ ‚Äî –Ω–∞ –Ω–∏—Ö –∑–∞–≤—è–∑–∞–Ω—ã handlers/webapp.
try:
    from app.config import (
        SPREADSHEET_URL,
        SAP_SHEET_NAME,
        USERS_SHEET_NAME,
        DATA_TTL,
        SEARCH_COLUMNS,
        TIMEZONE,
    )
except Exception:
    SPREADSHEET_URL = os.getenv("SPREADSHEET_URL", "")
    SAP_SHEET_NAME = os.getenv("SAP_SHEET_NAME", "SAP")
    USERS_SHEET_NAME = os.getenv("USERS_SHEET_NAME", "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    DATA_TTL = int(os.getenv("DATA_TTL", "600"))
    SEARCH_COLUMNS = os.getenv("SEARCH_COLUMNS", "").strip().split(",") if os.getenv("SEARCH_COLUMNS") else [
        "—Ç–∏–ø",
        "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
        "–∫–æ–¥",
        "oem",
        "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å",
        "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
        "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä",
    ]
    TIMEZONE = os.getenv("TIMEZONE", "Asia/Tashkent")

GOOGLE_APPLICATION_CREDENTIALS_JSON = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON", "")
SCOPES = ["https://www.googleapis.com/auth/spreadsheets"]

# ---------------- Runtime state ----------------
df: Optional[pd.DataFrame] = None
_last_load_ts: float = 0.0

_search_index: Dict[str, Set[int]] = {}

# –ò–Ω–¥–µ–∫—Å "–∫–æ–¥->image" –∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–∏ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ/–¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏).
# –ù–æ –ò–°–¢–ò–ù–ê –ø–æ —Ñ–æ—Ç–æ ‚Äî _image_file_index (–ø–æ –≤—Å–µ–º—É —Å—Ç–æ–ª–±—Ü—É image, –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞).
_image_index: Dict[str, str] = {}

# –ì–ª–∞–≤–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è —Ñ–æ—Ç–æ:
# KEY = basename(url) –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä UZ000664), VALUE = url –∏–∑ –°–¢–û–õ–ë–¶–ê image (–ª—é–±–æ–π —Å—Ç—Ä–æ–∫–∏)
_image_file_index: Dict[str, str] = {}

SHEET_ALLOWED: Set[int] = set()
SHEET_ADMINS: Set[int] = set()
SHEET_BLOCKED: Set[int] = set()

# —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å handlers.py
ASK_QUANTITY, ASK_COMMENT, ASK_CONFIRM = range(3)

# ---------------- Helpers: text / code ----------------
def _norm_code(x: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–¥–æ–≤/–Ω–æ–º–µ—Ä–æ–≤:
    - lower
    - O -> 0
    - –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ [a-z0-9]
    """
    s = str(x or "").strip().lower()
    s = s.replace("o", "0")
    s = re.sub(r"[^a-z0-9]", "", s)
    return s

def normalize(text: str) -> str:
    """–¥–ª—è webapp: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã"""
    return re.sub(r"[^\w\s]+", " ", str(text or "").lower(), flags=re.U).strip()

def squash(text: str) -> str:
    """–¥–ª—è webapp: —Å–∫–ª–µ–π–∫–∞ –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤/—Å–∏–º–≤–æ–ª–æ–≤"""
    return re.sub(r"[\W_]+", "", str(text or "").lower(), flags=re.U)

def _safe_col(df_: pd.DataFrame, col: str) -> Optional[pd.Series]:
    if df_ is None or col not in df_.columns:
        return None
    return df_[col].astype(str).fillna("").str.strip().str.lower()

def now_local_str() -> str:
    tz = ZoneInfo(TIMEZONE if TIMEZONE else "Asia/Tashkent")
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

def val(row: dict, key: str, default: str = "") -> str:
    return str(row.get(key, default) or default)

# ---------------- Formatting (card) ----------------
def format_row(row: dict) -> str:
    """
    –ö–æ–º–ø–∞–∫—Ç–Ω–∞—è –∫–∞—Ä—Ç–æ—á–∫–∞ (HTML) –ø–æ–¥ Telegram/mini app.
    """
    code        = val(row, "–∫–æ–¥").upper()
    name        = val(row, "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ")
    type_       = val(row, "—Ç–∏–ø")
    part_no     = val(row, "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä")
    oem_part    = val(row, "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä")
    qty         = val(row, "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ") or "‚Äî"
    price       = val(row, "—Ü–µ–Ω–∞")
    currency    = val(row, "–≤–∞–ª—é—Ç–∞")
    manuf       = val(row, "–∏–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å")
    oem         = val(row, "oem")

    lines: List[str] = []
    if code:
        lines.append(f"üî¢ <b>–ö–æ–¥:</b> {code}")
    if name:
        lines.append(f"üì¶ <b>–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ:</b> {name}")
    if type_:
        lines.append(f"üìé <b>–¢–∏–ø:</b> {type_}")
    if part_no:
        lines.append(f"üß© <b>–ü–∞—Ä—Ç ‚Ññ:</b> {part_no}")
    if oem_part:
        lines.append(f"‚öôÔ∏è <b>OEM ‚Ññ:</b> {oem_part}")
    lines.append(f"üì¶ <b>–ö–æ–ª-–≤–æ:</b> {qty}")
    if price or currency:
        lines.append(f"üí∞ <b>–¶–µ–Ω–∞:</b> {price} {currency}".rstrip())
    if manuf:
        lines.append(f"üè≠ <b>–ò–∑–≥–æ—Ç–æ–≤–∏—Ç–µ–ª—å:</b> {manuf}")
    if oem:
        lines.append(f"‚öôÔ∏è <b>OEM:</b> {oem}")

    return "\n".join(lines)

# ---------------- Google Sheets client ----------------
def get_gs_client():
    if not GOOGLE_APPLICATION_CREDENTIALS_JSON:
        raise RuntimeError("GOOGLE_APPLICATION_CREDENTIALS_JSON –Ω–µ –∑–∞–¥–∞–Ω")

    # –ø–æ–¥–¥–µ—Ä–∂–∫–∞: —Å—Ç—Ä–æ–∫–∞ JSON –∏–ª–∏ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
    try:
        info = json.loads(GOOGLE_APPLICATION_CREDENTIALS_JSON)
        creds = Credentials.from_service_account_info(info, scopes=SCOPES)
    except json.JSONDecodeError:
        creds = Credentials.from_service_account_file(GOOGLE_APPLICATION_CREDENTIALS_JSON, scopes=SCOPES)

    return gspread.authorize(creds)

# ---------------- Load SAP dataframe ----------------
def _load_sap_dataframe() -> pd.DataFrame:
    """
    –ë–µ—Ä—ë–º –¥–∞–Ω–Ω—ã–µ —Ç–∞–∫, –∫–∞–∫ –æ–Ω–∏ –≤ Google Sheets (get_all_values) ‚Äî –±–µ–∑ —Å—é—Ä–ø—Ä–∏–∑–æ–≤ –ø–æ —Ç–∏–ø–∞–º.
    –ó–∞–≥–æ–ª–æ–≤–∫–∏ -> lower.
    """
    if not SPREADSHEET_URL:
        raise RuntimeError("SPREADSHEET_URL –Ω–µ –∑–∞–¥–∞–Ω")

    client = get_gs_client()
    sh = client.open_by_url(SPREADSHEET_URL)
    ws = sh.worksheet(SAP_SHEET_NAME)

    values = ws.get_all_values()
    if not values:
        return pd.DataFrame()

    headers = [str(c).strip().lower() for c in values[0]]
    rows = values[1:]
    new_df = pd.DataFrame(rows, columns=headers)

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–∫–∞–∫ —Å—Ç—Ä–æ–∫–∏)
    for col in ("–∫–æ–¥", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem"):
        if col in new_df.columns:
            new_df[col] = new_df[col].astype(str).fillna("").str.strip()

    if "image" in new_df.columns:
        new_df["image"] = new_df["image"].astype(str).fillna("").str.strip()

    return new_df

# ---------------- Index builders ----------------
_ALLOWED_EXTS = {".png", ".jpg", ".jpeg", ".webp"}

def build_search_index(df_: pd.DataFrame) -> Dict[str, Set[int]]:
    """
    –ò–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    –¢–æ–∫–µ–Ω—ã –∏–∑ SEARCH_COLUMNS.
    """
    idx: Dict[str, Set[int]] = {}
    if df_ is None or df_.empty:
        return idx

    cols = [c for c in SEARCH_COLUMNS if c in df_.columns]

    for i, row in df_.iterrows():
        for c in cols:
            raw = str(row.get(c, "")).strip().lower()
            if not raw:
                continue

            # –¥–ª—è –∫–æ–¥–æ–≤/–Ω–æ–º–µ—Ä–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ü–µ–ª–∏–∫–æ–º
            if c in ("–∫–æ–¥", "–ø–∞—Ä—Ç –Ω–æ–º–µ—Ä", "oem –ø–∞—Ä—Ç –Ω–æ–º–µ—Ä"):
                key = _norm_code(raw)
                if key:
                    idx.setdefault(key, set()).add(i)

            # —Ç–æ–∫–µ–Ω—ã a-z0-9
            for t in re.findall(r"[a-z0-9]+", raw):
                t = _norm_code(t)
                if t:
                    idx.setdefault(t, set()).add(i)

    return idx

def build_image_index(df_: pd.DataFrame) -> Dict[str, str]:
    """
    –£—Å–∫–æ—Ä–µ–Ω–∏–µ: norm(–∫–æ–¥) -> URL –∏–∑ –ö–û–õ–û–ù–ö–ò image (–∏–∑ —Ç–æ–π –∂–µ —Å—Ç—Ä–æ–∫–∏).
    –í–ê–ñ–ù–û: —ç—Ç–æ—Ç –∏–Ω–¥–µ–∫—Å –ù–ï –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ —Å –∫–æ–¥–æ–º (–¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—à–∏–±–æ—á–Ω—ã).
    """
    out: Dict[str, str] = {}
    if df_ is None or df_.empty:
        return out
    if "–∫–æ–¥" not in df_.columns or "image" not in df_.columns:
        return out

    for _, row in df_.iterrows():
        code_raw = str(row.get("–∫–æ–¥", "")).strip()
        url = str(row.get("image", "")).strip()
        if not code_raw or not url:
            continue
        k = _norm_code(code_raw)
        if k:
            out.setdefault(k, url)
    return out

def build_image_file_index(df_: pd.DataFrame) -> Dict[str, str]:
    """
    –ì–õ–ê–í–ù–´–ô –∏–Ω–¥–µ–∫—Å —Ñ–æ—Ç–æ (–∫–∞–∫ —Ç—ã —Ç—Ä–µ–±—É–µ—à—å):
    –∏—â–µ–º –ø–æ –í–°–ï–ú —Å—Ç—Ä–æ–∫–∞–º —Å—Ç–æ–ª–±—Ü–∞ image —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞.

    –ü—Ä–∏–º–µ—Ä:
      URL: https://i.ibb.co/HLzjRrsQ/UZ000662.png
      key -> "UZ000662"
    """
    out: Dict[str, str] = {}
    if df_ is None or df_.empty or "image" not in df_.columns:
        return out

    for raw_url in df_["image"].astype(str).fillna("").tolist():
        u = str(raw_url).strip()
        if not u:
            continue
        try:
            path = urlparse(u).path
            fname = os.path.basename(path)
            name, ext = os.path.splitext(fname)
            if not name or ext.lower() not in _ALLOWED_EXTS:
                continue
            key = name.strip().upper()
            out.setdefault(key, u)  # –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –Ω–µ–ø—É—Å—Ç–æ–µ
        except Exception:
            continue

    return out

# ---------------- Reload/TTL ----------------
def ensure_fresh_data(force: bool = False):
    global df, _last_load_ts, _search_index, _image_index, _image_file_index

    need = force or df is None or (time.time() - _last_load_ts > DATA_TTL)
    if not need:
        return

    new_df = _load_sap_dataframe()
    df = new_df

    _search_index = build_search_index(df)
    _image_index = build_image_index(df)
    _image_file_index = build_image_file_index(df)

    _last_load_ts = time.time()

    logger.info(
        f"‚úÖ SAP reload: {len(df)} rows, index={len(_search_index)} keys, "
        f"images_by_row={len(_image_index)} keys, images_by_filename={len(_image_file_index)} keys"
    )

# ---------------- Strict image matching (by FULL COLUMN) ----------------
def find_image_url_by_code_strict(code: str) -> str:
    """
    –¢–í–û–Å –ø—Ä–∞–≤–∏–ª–æ:
    1) –±–µ—Ä—ë–º –∫–æ–¥ (–Ω–∞–ø—Ä–∏–º–µ—Ä UZ000664)
    2) –∏—â–µ–º –ø–æ –í–°–ï–ú —Å—Ç—Ä–æ–∫–∞–º —Å—Ç–æ–ª–±—Ü–∞ image URL, –≥–¥–µ basename –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è == –∫–æ–¥
    3) –µ—Å–ª–∏ –Ω–µ—Ç —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ (–ù–ï –ø–æ–¥—Å—Ç–∞–≤–ª—è–µ–º —á—É–∂–∏–µ —Ñ–æ—Ç–æ)
    """
    ensure_fresh_data()
    if df is None or df.empty:
        return ""

    code_raw = str(code or "").strip()
    if not code_raw:
        return ""

    url = _image_file_index.get(code_raw.upper(), "")
    return url or ""

async def find_image_by_code_async(code: str) -> str:
    """
    async wrapper –¥–ª—è handlers/webapp.
    """
    return find_image_url_by_code_strict(code)

# ---------------- URL resolve (ibb/drive) ----------------
def normalize_drive_url(url: str) -> str:
    """
    drive.google.com/file/d/<id> -> direct
    drive.google.com/open?id=<id> -> direct
    """
    u = str(url or "").strip()
    m = re.search(r"drive\.google\.com/(?:file/d/([-\w]{20,})|open\?id=([-\w]{20,}))", u)
    if not m:
        return u
    file_id = m.group(1) or m.group(2)
    return f"https://drive.google.com/uc?export=download&id={file_id}"

async def resolve_ibb_direct_async(url: str) -> str:
    """
    –µ—Å–ª–∏ –¥–∞–Ω–∞ –∫–æ—Ä–æ—Ç–∫–∞—è ibb.co/<id> ‚Äî –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å og:image (–ø—Ä—è–º—É—é i.ibb.co/..)
    –µ—Å–ª–∏ —É–∂–µ i.ibb.co ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    """
    u = str(url or "").strip()
    if not u:
        return ""
    try:
        if re.search(r"^https?://i\.ibb\.co/", u, re.I):
            return u
        if not re.search(r"^https?://ibb\.co/", u, re.I):
            return u

        async with aiohttp.ClientSession() as session:
            async with session.get(u, timeout=10) as resp:
                if resp.status != 200:
                    return u
                html = await resp.text()

        m = re.search(
            r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
            html,
            re.I
        )
        return m.group(1) if m else u
    except Exception as e:
        logger.warning(f"resolve_ibb_direct_async error: {e}")
        return u

async def resolve_image_url_async(url_raw: str) -> str:
    """
    –ø—Ä–∏–≤–æ–¥–∏–º –∫ –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–µ
    """
    if not url_raw:
        return ""
    u = normalize_drive_url(url_raw)
    u = await resolve_ibb_direct_async(u)
    return u or ""

# ---------------- Search core ----------------
def match_row_by_index(tokens: List[str]) -> Set[int]:
    """
    –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫:
    - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã
    - —Å–Ω–∞—á–∞–ª–∞ AND –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
    - –µ—Å–ª–∏ –ø—É—Å—Ç–æ ‚Äî OR –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    """
    ensure_fresh_data()
    if not tokens:
        return set()

    keys = [_norm_code(t) for t in tokens if t]
    keys = [k for k in keys if k]
    if not keys:
        return set()

    sets: List[Set[int]] = []
    for k in keys:
        s = _search_index.get(k, set())
        if not s:
            sets = []
            break
        sets.append(s)

    if sets:
        acc = sets[0].copy()
        for s in sets[1:]:
            acc &= s
        return acc

    found: Set[int] = set()
    for k in keys:
        found |= _search_index.get(k, set())
    return found

# ---------------- Users sheet (allowed/admin/blocked) ----------------
def _parse_int(x) -> Optional[int]:
    try:
        v = int(str(x).strip())
        return v if v > 0 else None
    except Exception:
        return None

def _dedupe_headers(headers: List[str]) -> List[str]:
    seen: Dict[str, int] = {}
    out: List[str] = []
    for i, h in enumerate(headers):
        base = re.sub(r"[^\w]+", "_", str(h or "").strip().lower()).strip("_")
        if not base:
            base = f"col{i+1}"
        if base not in seen:
            seen[base] = 1
            out.append(base)
        else:
            seen[base] += 1
            out.append(f"{base}_{seen[base]}")
    return out

def load_users_from_sheet() -> Tuple[Set[int], Set[int], Set[int]]:
    allowed: Set[int] = set()
    admins: Set[int] = set()
    blocked: Set[int] = set()

    try:
        client = get_gs_client()
        sh = client.open_by_url(SPREADSHEET_URL)
        ws = sh.worksheet(USERS_SHEET_NAME)
    except Exception:
        # –Ω–µ—Ç –ª–∏—Å—Ç–∞ ‚Äî –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º –Ω–∏–∫–æ–≥–æ
        return allowed, admins, blocked

    vals = ws.get_all_values()
    if not vals:
        return allowed, admins, blocked

    headers = _dedupe_headers(vals[0])
    rows = vals[1:]

    def truthy(v) -> bool:
        s = str(v).strip().lower()
        return s in ("1", "true", "–¥–∞", "yes", "y")

    for r in rows:
        rec = {headers[i]: (r[i] if i < len(r) else "") for i in range(len(headers))}
        uid = _parse_int(rec.get("user_id") or rec.get("id") or rec.get("uid"))
        if not uid:
            continue

        role = str(rec.get("role", "")).strip().lower()
        if role in ("admin", "–∞–¥–º–∏–Ω"):
            admins.add(uid); allowed.add(uid); continue
        if role in ("blocked", "ban", "–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω"):
            blocked.add(uid); continue

        if truthy(rec.get("blocked", "")):
            blocked.add(uid); continue
        if truthy(rec.get("admin", "")):
            admins.add(uid); allowed.add(uid); continue
        if "allowed" in rec and truthy(rec.get("allowed", "")):
            allowed.add(uid); continue

        # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä–∞–∑—Ä–µ—à–∞–µ–º
        allowed.add(uid)

    return allowed, admins, blocked

# ---------------- Initial load ----------------
def initial_load():
    ensure_fresh_data(force=True)
    try:
        a, ad, b = load_users_from_sheet()
        SHEET_ALLOWED.clear(); SHEET_ALLOWED.update(a)
        SHEET_ADMINS.clear(); SHEET_ADMINS.update(ad)
        SHEET_BLOCKED.clear(); SHEET_BLOCKED.update(b)
        logger.info(f"‚úÖ USERS reload: allowed={len(SHEET_ALLOWED)} admins={len(SHEET_ADMINS)} blocked={len(SHEET_BLOCKED)}")
    except Exception as e:
        logger.warning(f"USERS load failed: {e}")

# async helper
import asyncio
async def asyncio_to_thread(func, *args, **kwargs):
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(None, lambda: func(*args, **kwargs))

async def initial_load_async():
    await asyncio_to_thread(ensure_fresh_data, True)
    try:
        a, ad, b = await asyncio_to_thread(load_users_from_sheet)
        SHEET_ALLOWED.clear(); SHEET_ALLOWED.update(a)
        SHEET_ADMINS.clear(); SHEET_ADMINS.update(ad)
        SHEET_BLOCKED.clear(); SHEET_BLOCKED.update(b)
        logger.info(f"‚úÖ USERS reload: allowed={len(SHEET_ALLOWED)} admins={len(SHEET_ADMINS)} blocked={len(SHEET_BLOCKED)}")
    except Exception as e:
        logger.warning(f"USERS load failed: {e}")
'''
out_path = Path("/mnt/data/data.py")
out_path.write_text(data_py, encoding="utf-8")
str(out_path), out_path.stat().st_size
